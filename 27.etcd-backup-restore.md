# ETCD backup and restore

Letâ€™s go step-by-step and cover:
1ï¸âƒ£ What etcd is and why itâ€™s important
2ï¸âƒ£ Why we need backup and restore
3ï¸âƒ£ How backup and restore work (concept + commands)
4ï¸âƒ£ What is actually captured in the backup
5ï¸âƒ£ Common CKA-related scenarios

---

## ğŸ§  1. What is etcd?

* **etcd** is a **key-value store** used by Kubernetes to store all **cluster state and configuration data**.
* Itâ€™s like the **brain of your cluster** â€” everything about nodes, pods, secrets, configmaps, deployments, RBAC, and more lives here.
* If etcd is lost, **the entire cluster state is gone**, even if worker nodes are still running.

### Example:

When you run:

```bash
kubectl get pods -A
```

â†’ The API Server retrieves data from etcd.

---

## âš ï¸ 2. Why Backup and Restore are Required

* To **recover the cluster** in case of:

  * etcd data corruption
  * control-plane node failure
  * accidental deletion of cluster objects
  * etcd upgrade/migration issues
* Backup ensures **cluster state consistency** and allows restoring to a known good point.

### Real-world Example:

If an admin accidentally deletes all namespaces except `kube-system`, restoring from an etcd snapshot brings back the full previous state.

---

## ğŸ§© 3. How Backup and Restore Work

### ğŸ—‚ Backup = Snapshot of etcd database

#### âœ… Step 1: Identify etcd pod

In most clusters:

```bash
kubectl get pods -n kube-system
```

Example:

```
etcd-controlplane
```

#### âœ… Step 2: Get etcd container details

```bash
kubectl -n kube-system exec etcd-controlplane -- etcdctl version
```

#### âœ… Step 3: Run snapshot save command

Inside the control-plane (or directly via SSH):

```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup.db
```

**Output:**

```
Snapshot saved at /opt/etcd-backup.db
```

You can verify:

```bash
ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup.db
```

---

### ğŸ”„ Restore = Rebuild etcd from the snapshot

#### âœ… Step 1: Stop kube-apiserver (to prevent writes)

* In a kubeadm cluster, **control plane components (kube-apiserver, kube-controller-manager, kube-scheduler, etcd)** run as **static pods**, **not systemd services**.
* That means there is **no `kube-apiserver.service`**, so `systemctl` canâ€™t stop it.

You can verify:

```bash
ps -ef | grep kube-apiserver
```

or

```bash
kubectl get pods -n kube-system -o wide | grep kube-apiserver
```

Youâ€™ll see itâ€™s running as a **pod**, managed by **kubelet**.

---

## ** How to stop kube-apiserver (or other control plane components)**

Since they are **static pods**, you stop them by **stopping kubelet**:

```bash
sudo systemctl stop kubelet
```

* This stops all control plane components temporarily.
* After your restore is done, start kubelet again:

```bash
sudo systemctl start kubelet
```

* kubelet will automatically start the static pods again from `/etc/kubernetes/manifests/`.

---

## **3ï¸âƒ£ Key points for CKA**

* **Static pods** are stored in `/etc/kubernetes/manifests/`.
* You **never use systemctl** for individual components on a kubeadm cluster.
* To simulate downtime for etcd restore, **stop kubelet**, replace etcd data, then **start kubelet**.



#### âœ… Step 2: Restore snapshot to a new directory

```bash
ETCDCTL_API=3 etcdctl snapshot restore /opt/etcd-backup.db \
  --data-dir /var/lib/etcd-restore
```

#### âœ… Step 3: Update etcd configuration

Edit `/etc/kubernetes/manifests/etcd.yaml`:

```yaml
- --data-dir=/var/lib/etcd-restore
```

Then kubelet will automatically restart the etcd pod using the new data directory.

#### âœ… Step 4: Verify cluster state

Once the control plane is back up:

```bash
kubectl get pods -A
```

You should see your old state restored.

---

## ğŸ“¦ 4. What is Included in etcd Backup

The etcd snapshot contains **the entire cluster state**, including:

| Type                  | Examples                                      |
| --------------------- | --------------------------------------------- |
| Cluster Configuration | Nodes, Namespaces, Deployments                |
| Security              | Secrets, ServiceAccounts, Roles, RoleBindings |
| Network               | Services, Endpoints, Network Policies         |
| Custom Resources      | CRDs, Custom Objects                          |
| Storage Metadata      | PVs, PVCs (metadata only, *not* actual data)  |

> **Note:** PV data (like actual files in NFS, EBS, etc.) must be backed up separately.

---

## ğŸ’¡ 5. CKA Exam Tip â€” Common Scenarios

### ğŸ§© Scenario 1: Take snapshot of etcd

> â€œTake an etcd backup and store it at `/opt/snapshot-pre-restore.db`.â€

**Solution:**

```bash
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/snapshot-pre-restore.db
```

### ğŸ§© Scenario 2: Restore etcd from a snapshot

> â€œRestore etcd from snapshot `/opt/snapshot-pre-restore.db`.â€

**Solution:**

```bash
systemctl stop kube-apiserver
ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-restore.db \
  --data-dir /var/lib/etcd-from-backup
# edit etcd.yaml to point data-dir to /var/lib/etcd-from-backup
# kubelet auto-restarts etcd
```

### ğŸ§© Scenario 3: Verify snapshot content

```bash
ETCDCTL_API=3 etcdctl snapshot status /opt/snapshot-pre-restore.db
```

---

## ğŸ§­ Summary Table

| Action         | Command                                            | Description            |
| -------------- | -------------------------------------------------- | ---------------------- |
| Take Backup    | `etcdctl snapshot save <path>`                     | Creates snapshot       |
| Check Backup   | `etcdctl snapshot status <path>`                   | View snapshot metadata |
| Restore Backup | `etcdctl snapshot restore <path> --data-dir <dir>` | Restores snapshot data |
| Verify Restore | `kubectl get nodes`                                | Check cluster state    |

---

Letâ€™s now go **step-by-step exactly as weâ€™ll do it in the CKA exam**, using a **kubeadm-based control plane node** where etcd runs as a static pod.

Weâ€™ll simulate both **backup** and **restore** from start to finish.

---

## ğŸ§± Scenario Overview

**Goal:**
Take an etcd backup and then restore it on the same control-plane node.

**Cluster Type:** Kubeadm-based single control plane.
**Pod:** `etcd-controlplane` (name may vary slightly, e.g., `etcd-master` or `etcd-<hostname>`).

---

## âš™ï¸ Step 1: Get into Control Plane Node

In CKA exam, youâ€™ll typically be told:

> â€œPerform the task on controlplane node.â€

Switch to it:

```bash
ssh controlplane
```

Or, if already on it:

```bash
hostname
```

You should see something like `controlplane`.

---

## ğŸ§© Step 2: Locate etcd Pod and Certificates

Run:

```bash
kubectl get pods -n kube-system | grep etcd
```

Example output:

```
etcd-controlplane   1/1   Running   0   3d5h
```

Now get details of etcd manifest file:

```bash
cat /etc/kubernetes/manifests/etcd.yaml | grep "\-\-"
```

Look for:

```
--cert-file=/etc/kubernetes/pki/etcd/server.crt
--key-file=/etc/kubernetes/pki/etcd/server.key
--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
--data-dir=/var/lib/etcd
```

---

## ğŸ’¾ Step 3: Take etcd Backup (Snapshot)

Run from control plane node shell:

```bash
export ETCDCTL_API=3
etcdctl snapshot save /opt/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```

âœ… Output:

```
Snapshot saved at /opt/etcd-backup.db
```

---

## ğŸ” Step 4: Verify Snapshot File

Check its metadata:

```bash
etcdctl snapshot status /opt/etcd-backup.db -w table
```

Expected output:

```
+----------+----------+------------+------------+
|  TOTAL   | HASH     | REVISION   |  SIZE      |
+----------+----------+------------+------------+
|    1234  | abcd123  |  100       |  3.1 MB    |
+----------+----------+------------+------------+
```

This confirms the snapshot is valid.

---

## ğŸ§¨ Step 5: Simulate Data Loss (Optional Demo)

Letâ€™s pretend something was deleted:

```bash
kubectl delete namespace dev
```

or

```bash
kubectl delete pod nginx -n default
```

---

## ğŸ§° Step 6: Restore the etcd from Backup

1ï¸âƒ£ **Stop kube-apiserver** to avoid new writes:

```bash
systemctl stop kube-apiserver
```

2ï¸âƒ£ **Restore snapshot to a new data directory:**

```bash
export ETCDCTL_API=3
etcdctl snapshot restore /opt/etcd-backup.db \
  --data-dir /var/lib/etcd-from-backup
  
```

Output:

```
{"level":"info","msg":"restoring snapshot","path":"/opt/etcd-backup.db"}
```

---

## ğŸ§¾ Step 7: Point etcd Pod to Restored Data

Edit static pod manifest:

```bash
vi /etc/kubernetes/manifests/etcd.yaml
```

Find the line:

```
--data-dir=/var/lib/etcd
```

Change it to:

```
--data-dir=/var/lib/etcd-from-backup
```

And in volumes and volumemounts as well

<img width="897" height="354" alt="image" src="https://github.com/user-attachments/assets/a23f4347-b654-48cc-8083-901459d8198c" />
<img width="897" height="354" alt="image" src="https://github.com/user-attachments/assets/38700608-597e-4b91-b62f-1b14b836ae5e" />


Save and exit.
ğŸ“Œ **Kubelet automatically detects the change** and restarts the etcd pod using the new directory.

---

## ğŸš€ Step 8: Verify Restore

Once etcd and kube-apiserver pods are running again, check:

```bash
kubectl get pods -A
```

You should see the cluster state as it was at the backup time â€” e.g., deleted namespaces or pods reappear.

---

## ğŸ§­ Step 9: (Optional) Restart kube-apiserver Manually if Needed

If it didnâ€™t auto-start:

```bash
systemctl start kube-apiserver
```

---

## âœ… Step 10: Confirm Cluster Health

```bash
kubectl get nodes
kubectl get pods -A
```

And check etcd cluster health:

```bash
etcdctl endpoint health \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```

Output:

```
https://127.0.0.1:2379 is healthy
```

---

[Kubernetes Official Link - etcd backup & Restore](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)

---

## ğŸ§  Summary â€” Key Commands for CKA

| Purpose           | Command                                                                             |
| ----------------- | ----------------------------------------------------------------------------------- |
| Backup etcd       | `etcdctl snapshot save /opt/etcd-backup.db`                                         |
| Verify snapshot   | `etcdctl snapshot status /opt/etcd-backup.db -w table`                              |
| Restore snapshot  | `etcdctl snapshot restore /opt/etcd-backup.db --data-dir /var/lib/etcd-from-backup` |
| Check etcd health | `etcdctl endpoint health`                                                           |

---


